{
    "collab_server" : "",
    "contents" : "\n\n#' Import of variables stored in the parameters file and of the dataset to quantify\n#'\n#' @param parameters_path CSV file woth input paths and desired pre-processing steps\n#'\n#' @return Imported data of experiment\n#' @export import_data\n#' @import data.table\n#'\n#' @examples\n#' setwd(paste(system.file(package = \"rDolphin\"),\"extdata\",sep='/'))\n#' imported_data=import_data(\"Parameters_MTBLS242_15spectra_5groups.csv\")\n\nimport_data = function(parameters_path) {\n  #Created by Daniel Canueto 30/08/2016\n  print('Importing necessary data to begin the profiling. The more spectra and narrower bucketing, the more time I need.')\n\n\n  #List of parameters to use to create the dataset\n  params = list()\n\n  #Import of parameters from the csv file\n  import_profile = read.delim(\n    parameters_path,\n    sep = ',',\n    header = T,\n    stringsAsFactors = F\n  )\n  import_profile = as.data.frame(sapply(import_profile, function(x)\n    gsub(\"\\\\\\\\\", \"/\", x)))\n\n  #Getting the names of experiments, signals and ROIs to quantify and use\n  metadata_path = as.character(import_profile[3, 2])\n\n  dummy = read.delim(\n    metadata_path,\n    sep = ',',\n    header = T,\n    stringsAsFactors = F\n  )\n  Experiments=as.character(dummy[,1])\n  Experiments = as.vector(Experiments[Experiments != ''])\n  Metadata=dummy[,-1,drop=F]\n\n  #Import of ROI profiles and generation of names and codes of signals\n  ROI_data=try(read.csv(as.character(import_profile[6, 2]), stringsAsFactors = F),silent=T)\n  signals_names=paste(ROI_data[which(!is.na(ROI_data[, 1])),4],ROI_data[which(!is.na(ROI_data[, 1])),5],sep='_')\n  signals_codes = 1:length(signals_names)\n\n\n#Other necessary variables\n  freq = as.numeric(as.character(import_profile[10, 2]))\n  biofluid=import_profile[12, 2]\n  jres_path=as.character(import_profile[13, 2])\n  try(source(as.character(import_profile[14, 2])),silent=T)\n  if (!exists(\"program_parameters\")) program_parameters=fitting_variables()\n\n#Creation of repository adapted to biofluid\n  repository=data.frame(fread(file.path(system.file(package = \"rDolphin\"),\"extdata\",\"HMDB_Repository.csv\")))\n  biofluid_column=which(colnames(repository)==biofluid)\n  repository=repository[!is.na(repository[,biofluid_column]),]\n  repository=repository[repository[,biofluid_column]>0,]\n  repository=repository[sort(repository[,biofluid_column],decreasing = T,index.return=T)$ix,c(1:3,5:7,biofluid_column)]\n\n\n\n  #Kind of normalization\n  normalization = import_profile[7, 2]\n  pqn='N'\n\n  params$norm_AREA = 'N'\n  params$norm_PEAK = 'N'\n  params$norm_left_ppm = program_parameters$spectrum_borders[1]\n  params$norm_right_ppm = program_parameters$spectrum_borders[2]\n  if (normalization == 1) {\n    #Eretic\n    params$norm_AREA = 'Y'\n    params$norm_left_ppm = 11.53\n    params$norm_right_ppm = 10.47\n  } else if (normalization == 2) {\n    #TSP\n    params$norm_AREA = 'Y'\n    params$norm_left_ppm = 0.1\n    params$norm_right_ppm = -0.1\n  } else if (normalization == 3) {\n    #Creatinine (intensity, not area, maybe dangerous for rats because of oxalacetate)\n    params$norm_PEAK = 'Y'\n    params$norm_left_ppm = 3.10\n    params$norm_right_ppm = 3\n  } else if (normalization == 4) {\n    #Spectrum AreA\n    params$norm_AREA = 'Y'\n  } else if (normalization == 0) {\n    #No normailzation\n\n  } else if (normalization == 5) {\n    #PQN normailzation\n    params$norm_AREA = 'Y'\n    pqn='Y'\n  }\n\n  #Alignment\n  alignment = import_profile[8, 2]\n  params$glucose_alignment = 'N'\n  params$tsp_alignment = 'N'\n  params$peak_alignment = 'N'\n  params$ref_peak_pos = 8.452\n  if (alignment == 1) {\n    #Glucose\n    params$glucose_alignment = 'Y'\n  } else if (alignment == 2) {\n    #TSP\n    params$tsp_alignment = 'Y'\n  } else if (alignment == 3) {\n    #Formate\n    params$peak_alignment = 'Y'\n  }\n\n  #Suppresion regions\n  suppression = as.character(import_profile[9, 2])\n  if (suppression == '') {\n    params$disol_suppression = 'N'\n  } else {\n    params$disol_suppression = 'Y'\n    params$disol_suppression_ppm = as.numeric(strsplit(suppression, '-|;')[[1]])\n    params$disol_suppression_ppm = matrix(params$disol_suppression_ppm,length(params$disol_suppression_ppm) /2,2,byrow = T)\n  }\n\n  #Variables only necessary for reading Bruker files\n  bruker_path = import_profile[1, 2]\n  expno = as.character(import_profile[4, 2])\n  processingno = as.character(import_profile[5, 2])\n\n  #Variables only necessary for reading dataset in csv format\n  dataset_path = as.character(import_profile[2, 2])\n\n  #If data comes from csv dataset\n  if (bruker_path == '' || expno == '' || processingno == '') {\n    if (dataset_path != '') {\n      #Reading of dataset file (ideally with fread of data.table package, but seems that the package is not compatible with R 3.3.1). Maybe now it is possible.\n      imported_data = list()\n      dummy = data.matrix(fread(dataset_path, sep = ',',header=F))\n      notnormalizeddataset=imported_data$dataset=dummy[-1,]\n      imported_data$dataset[is.na(imported_data$dataset)]=0\n\t  imported_data$ppm = round(dummy[1,],4)\n      if (imported_data$ppm[1]<imported_data$ppm[2]) {\n        imported_data$dataset=t(apply(imported_data$dataset,1,rev))\n        imported_data$ppm=rev(imported_data$ppm)\n      }\n\t  colnames(imported_data$dataset) = imported_data$ppm\n\t  rownames(imported_data$dataset) = Experiments\n\t  params$buck_step = ifelse(\n\t    as.character(import_profile[11, 2]) == '',\n\t    abs(imported_data$ppm[1] - imported_data$ppm[length(imported_data$ppm)]) /\n\t      length(imported_data$ppm),\n\t    as.numeric(as.character(import_profile[11, 2]))\n\t  )\n\n\t#TODO: revise alignment and normalization when coming data from csv.\n      if (alignment == 1) {\n        #Glucose\n        lmn=apply(imported_data$dataset,1,function(x)JTPcalibrateToGlucose(x,imported_data$ppm)$deltaPPM)\n        } else if (alignment == 2) {\n        #TSP\n          lmn=apply(imported_data$dataset,1,function(x)JTPcalibrateToTSP(x,imported_data$ppm)$deltaPPM)\n          } else if (alignment == 3) {\n        #Formate\n            lmn=apply(imported_data$dataset,1,function(x)JTPcalibrateToPeak(x,imported_data$ppm,params$ref_peak_pos)$deltaPPM)\n          }\n            if (alignment!=0&&nrow(imported_data$dataset)>1) {\n\n              imported_data$ppm=imported_data$ppm-median(lmn)\n\n              spectra_lag=round((lmn-median(lmn))/params$buck_step)\n\n      so=(1+max(abs(spectra_lag))):(length(imported_data$ppm)-max(abs(spectra_lag)))\n      for (i in 1:dim(imported_data$dataset)[1])   imported_data$dataset[i,so-spectra_lag[i]]=imported_data$dataset[i,so]\n            }\n\t  norm_factor=rep(1,nrow(imported_data$dataset))\n      if (params$norm_AREA == 'Y') {\n        # for (i in 1:dim(imported_data$dataset)[1])\n        #   norm_factor[i]=mean(rowSums(imported_data$dataset[,which.min(abs(imported_data$ppm-params$norm_left_ppm)):which.min(abs(imported_data$ppm-params$norm_right_ppm))]))/sum(imported_data$dataset[i,which.min(abs(imported_data$ppm-params$norm_left_ppm)):which.min(abs(imported_data$ppm-params$norm_right_ppm))])\n        norm_factor=rowSums(imported_data$dataset[,which.min(abs(imported_data$ppm-params$norm_left_ppm)):which.min(abs(imported_data$ppm-params$norm_right_ppm))])\n      } else if (params$norm_PEAK == 'Y') {\n        norm_factor=apply(imported_data$dataset[,which.min(abs(imported_data$ppm-params$norm_left_ppm)):which.min(abs(imported_data$ppm-params$norm_right_ppm))],1,max)\n      }\n\t  imported_data$dataset=imported_data$dataset/norm_factor\n    } else {\n      print('Problem when creating the dataset. Please revise the parameters.')\n      return()\n    }\n\n  } else {\n\n    #Reading of Bruker files\n    params$dir = bruker_path\n    params$expno = expno\n    params$processingno = processingno\n    params$buck_step = as.numeric(as.character(import_profile[11,2]))\n    imported_data = Metadata2Buckets(Experiments, params,program_parameters$spectrum_borders)\n    Metadata=Metadata[!Experiments %in% imported_data$not_loaded_experiments,]\n    Experiments=Experiments[!Experiments %in% imported_data$not_loaded_experiments]\n    norm_factor=imported_data$norm_factor\n    notnormalizeddataset=imported_data$dataset*norm_factor\n\n\n    # if (dim(imported_data$dataset)==2) dummy=NA\n  }\n\n  imported_data$dataset[is.na(imported_data$dataset)]=min(abs(imported_data$dataset)[abs(imported_data$dataset)>0])\n\n  #Region suppression\n    if (params$disol_suppression == 'Y') {\n      ind=c()\n      for (i in seq(nrow(params$disol_suppression_ppm))) ind=c(ind,which.min(abs(imported_data$ppm-params$disol_suppression_ppm[i,1])):which.min(abs(imported_data$ppm-params$disol_suppression_ppm[i,2])))\n      imported_data$dataset=imported_data$dataset[,-ind,drop=F]\n      imported_data$ppm=imported_data$ppm[-ind]\n    }\n\n# Possibility of removing zones without interesting information. Added in the future.\n #snr=apply(imported_data$dataset,1,function(x)stats::mad(x,na.rm=T))\n\n  # ind=seq(1,ncol(imported_data$dataset),round(ncol(imported_data$dataset)/(0.1/params$buck_step)))\n  # flan=rep(NA,length(ind))\n  # for (i in 1:length(ind)) flan[i]=tryCatch(colMeans(imported_data$dataset[,ind[i]:(ind[i]+1)]), error=function(e) NA)\n  #\n  # snr=apply(imported_data$dataset[,ind[which.min(flan)]:ind[which.min(flan)+1]],1,function(x)stats::mad(x,na.rm=T))\n  #\n  # dfg=matrix(0,nrow(imported_data$dataset),ncol(imported_data$dataset))\n  # for (i in 1:nrow(imported_data$dataset)) {\n  #   dfg[i,which(imported_data$dataset[i,]>snr[i])]=1\n  # }\n  #\n  # dfi=which(apply(dfg,2,sum)>0.5*nrow(imported_data$dataset))\n  # dfj=c()\n  # for (i in 1:length(dfi)) dfj=unique(c(dfj,round((dfi[i]-0.02/params$buck_step):(dfi[i]+0.02/params$buck_step))))\n  # dfj = dfj[dfj > 0 & dfj <= ncol(imported_data$dataset)]\n  #imported_data$dataset=imported_data$dataset[,dfj,drop=F]\n  #imported_data$ppm=imported_data$ppm[dfj]\n\n  #If pqn is desired\n  #TODO: specify which are the control samples or if there are no control samples.\n  if (pqn=='Y'&&nrow(imported_data$dataset)>1) {\n    treated=t(imported_data$dataset[,which(apply(imported_data$dataset,2,median)>median(apply(imported_data$dataset,2,median))),drop=F])\n    reference <- apply(treated,1,function(x)median(x,na.rm=T))\n    quotient <- treated/reference\n    quotient.median <- apply(quotient,2,function(x)median(x,na.rm=T))\n    imported_data$dataset <- imported_data$dataset/quotient.median\n    norm_factor=norm_factor*quotient.median\n\n  }\n\n  #Adaptation of data to magnitudes similar to 1. To be removed in the future.\n  secondfactor=quantile(imported_data$dataset,0.9,na.rm=T)\n  imported_data$dataset=imported_data$dataset/secondfactor\n  norm_factor=norm_factor*secondfactor\n\n\n  imported_data$dataset=imported_data$dataset[,which(apply(imported_data$dataset,2,function(x) all(is.na(x)))==F),drop=F]\n  imported_data$dataset[is.na(imported_data$dataset)]=0\n\n  imported_data$ppm=imported_data$ppm[which(!is.na(imported_data$ppm))]\n  # if (imported_data$ppm[1]<imported_data$ppm[2]) {\n    # imported_data$ppm=rev(imported_data$ppm)\n    # imported_data$dataset=t(apply(imported_data$dataset,1,rev))\n  # }\n  #Storage of parameters needed to perform the fit in a single variable to return.\n\n  imported_data$buck_step = params$buck_step\n  imported_data$metadata_path = metadata_path\n  imported_data$parameters_path = parameters_path\n  imported_data$signals_names = signals_names\n  imported_data$signals_codes = signals_codes\n  imported_data$Experiments = Experiments\n  imported_data$ROI_data = ROI_data\n  imported_data$freq = freq\n  imported_data$Metadata=Metadata\n  imported_data$repository=repository\n  imported_data$jres_path=jres_path\n  imported_data$program_parameters=program_parameters\n  imported_data$export_path=export_path\n\n\n  #creation of list with the different final outputs\n  dummy=matrix(NaN,nrow(imported_data$dataset),length(imported_data$signals_names),dimnames=list(imported_data$Experiments,imported_data$signals_names))\n  imported_data$final_output = list(quantification= dummy,signal_area_ratio = dummy,fitting_error = dummy, shift = dummy,intensity = dummy, half_band_width = dummy)\n\n  #creation of list of necessary parameters to load quantifications and evaluate quality of them\n  imported_data$useful_data=vector('list',length(imported_data$Experiments))\n  for (i in seq_along(imported_data$useful_data)) imported_data$useful_data[[i]]=vector('list',length(imported_data$signals_codes))\n  for (i in seq_along(imported_data$useful_data)) {\n    for (j in seq_along(imported_data$useful_data[[i]])) {\n      imported_data$useful_data[[i]][[j]]=list(Ydata=NULL,Xdata=NULL,ROI_profile=NULL,program_parameters=NULL,plot_data=NULL,FeaturesMatrix=NULL,signals_parameters=NULL,results_to_save=NULL,error1=1000000)\n    }}\n\n\t# dummy = which(is.na(imported_data$ROI_data[, 1]))\n    # if (length(dummy)==0) dummy=dim(imported_data$ROI_data)[1]+1\n    # lal=which(duplicated(imported_data$ROI_data[-dummy,1:2])==F)\n    # imported_data$ROI_separator = cbind(lal, c(lal[-1] - 1, dim(imported_data$ROI_data[-dummy,])[1]))\n  export_path=file.path(dirname(as.character(import_profile[6, 2])),\"input_data\")\n  #Useful data about conditions of import of data. TO BE REARRANGED\n  dir.create(export_path,showWarnings = FALSE)\n  # fwrite(as.data.frame(imported_data$params),file=file.path(imported_data$export_path, 'initial_params.csv'))\n  fwrite(as.data.frame(imported_data$dataset),file=file.path(export_path, 'initial_dataset.csv'))\n  fwrite(as.data.frame(notnormalizeddataset),file=file.path(export_path, 'notnormalizeddataset.csv'))\n  fwrite(as.data.frame(norm_factor),file=file.path(export_path, 'norm_factor.csv'))\n  if (\"not_loaded_experiments\" %in% names(imported_data)&&length(imported_data$not_loaded_experiments)>0)\n    fwrite(as.data.frame(imported_data$not_loaded_experiments),file=file.path(export_path, 'not_loaded_experiments.csv'),col.names = F)\n  print('Done!')\n  return(imported_data)\n\n}\n",
    "created" : 1493635077205.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3647970603",
    "id" : "9BF55AAE",
    "lastKnownWriteTime" : 1493970099,
    "last_content_update" : 1493970099,
    "path" : "~/GitHub/rDolphin/R/import_data.R",
    "project_path" : "R/import_data.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}